{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyaqGt2jWZT0wX+oyHtehq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/klei22/nanoGPT/blob/master/llm_size_calculations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahPT8yev5GKI"
      },
      "outputs": [],
      "source": [
        "def transformer_count_params(d_model=512, d_ff=2048, encoder=True, approx=False):\n",
        "    \"\"\"\n",
        "    Calculate the number of parameters in Transformer Encoder/Decoder.\n",
        "    Formulas are the following:\n",
        "        multi-head attention: 4*(d_model^2 + d_model)\n",
        "            if approx=False, 4*d_model^2 otherwise\n",
        "        feed-forward: 2*d_model*d_ff + d_model + d_ff\n",
        "            if approx=False, 2*d_model*d_ff otherwise\n",
        "        layer normalization: 2*d_model if approx=False, 0 otherwise\n",
        "\n",
        "    Encoder block consists of:\n",
        "        1 multi-head attention block,\n",
        "        1 feed-forward net, and\n",
        "        2 layer normalizations.\n",
        "    Decoder block consists of:\n",
        "        2 multi-head attention blocks,\n",
        "        1 feed-forward net, and\n",
        "        3 layer normalizations.\n",
        "\n",
        "    :param d_model: (int) model dimensionality\n",
        "    :param d_ff: (int) internal dimensionality of a feed-forward neural network\n",
        "    :param encoder: (bool) if True, return the number of parameters of the Encoder,\n",
        "        otherwise the Decoder\n",
        "    :param approx: (bool) if True, result is approximate (see formulas)\n",
        "    :return: (int) number of learnable parameters in Transformer Encoder/Decoder\n",
        "    \"\"\"\n",
        "\n",
        "    attention = 4 * (d_model ** 2 + d_model) if not approx else 4 * d_model ** 2\n",
        "    feed_forward = 2 * d_model * d_ff + d_model + d_ff if not approx else 2 * d_model * d_ff\n",
        "    layer_norm = 2 * d_model if not approx else 0\n",
        "\n",
        "    return attention + feed_forward + 2 * layer_norm \\\n",
        "        if encoder else 2 * attention + feed_forward + 3 * layer_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3O1bj_DD5SaF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RisZew025ST-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_count_params(d_model=384, d_ff=2048, encoder=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tClyGKHs5HQT",
        "outputId": "cc576a28-188c-4ce4-ad9e-e97f2ca9d2c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2760320"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 256\n",
        "    vocab_size: int = 50304  # Adjust as needed\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int = 384\n",
        "    dropout: float = 0.1\n",
        "    softmax_variant_attn: str = \"softmax\"\n",
        "    softmax_variant_output: str = \"softmax\"\n",
        "    use_abs_pos_embeddings: bool = False\n",
        "    use_rotary_embeddings: bool = True\n",
        "    rope_variant: str = \"rope\"\n",
        "    shortrope_length: int = 8\n",
        "    use_post_ln: bool = True\n",
        "    use_pre_ln: bool = False\n",
        "    layernorm_variant: str = \"rmsnorm\"\n",
        "    bias: bool = False\n",
        "    activation_variant: str = \"gelu\"\n",
        "\n",
        "def calculate_parameters(config: GPTConfig) -> int:\n",
        "    # Token embedding parameters\n",
        "    token_embedding_params = config.vocab_size * config.n_embd\n",
        "    \n",
        "    # Positional embedding parameters (if using absolute position embeddings)\n",
        "    position_embedding_params = config.block_size * config.n_embd if config.use_abs_pos_embeddings else 0\n",
        "    \n",
        "    # Transformer parameters\n",
        "    transformer_params = 0\n",
        "    for _ in range(config.n_layer):\n",
        "        # Layer normalization parameters (assuming 2 per block for pre and post-layernorm)\n",
        "        ln_params = 2 * config.n_embd * 2  # Two layer norms per block, scale and bias\n",
        "        \n",
        "        # Self-attention parameters\n",
        "        attn_params = (config.n_embd * 3 * config.n_embd) + (config.n_embd * config.n_embd)  # QKV projections and output projection\n",
        "        if config.bias:\n",
        "            attn_params += 4 * config.n_embd  # Adding bias terms for QKV and output projection\n",
        "        \n",
        "        # MLP parameters\n",
        "        mlp_params = (config.n_embd * 4 * config.n_embd) + (4 * config.n_embd * config.n_embd)  # FC and proj layers\n",
        "        if config.bias:\n",
        "            mlp_params += 2 * 4 * config.n_embd  # Adding bias terms for FC and proj layers\n",
        "        \n",
        "        transformer_params += ln_params + attn_params + mlp_params\n",
        "    \n",
        "    # Total parameters\n",
        "    total_params = token_embedding_params + position_embedding_params + transformer_params\n",
        "    return total_params\n",
        "\n",
        "# Example usage\n",
        "config = GPTConfig(n_layer=6, n_embd=)  # Example configuration\n",
        "total_params = calculate_parameters(config)\n",
        "print(f\"Total Parameters: {total_params}\")\n"
      ],
      "metadata": {
        "id": "_W-lw47v5I1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304  # Vocabulary size\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.1\n",
        "    softmax_variant_attn: str = \"softmax\"\n",
        "    softmax_variant_output: str = \"softmax\"\n",
        "    use_abs_pos_embeddings: bool = False\n",
        "    use_rotary_embeddings: bool = True\n",
        "    rope_variant: str = \"rope\"\n",
        "    shortrope_length: int = 8\n",
        "    use_post_ln: bool = True\n",
        "    use_pre_ln: bool = False\n",
        "    layernorm_variant: str = \"rmsnorm\"\n",
        "    bias: bool = False\n",
        "    activation_variant: str = \"gelu\"\n",
        "\n",
        "def calculate_parameters(config: GPTConfig) -> int:\n",
        "    # Token embedding parameters (includes vocabulary size in the computation)\n",
        "    token_embedding_params = config.vocab_size * config.n_embd\n",
        "    \n",
        "    # Positional embedding parameters (if using absolute position embeddings)\n",
        "    position_embedding_params = config.block_size * config.n_embd if config.use_abs_pos_embeddings else 0\n",
        "    \n",
        "    # Transformer parameters (calculating parameters for all layers)\n",
        "    transformer_params = 0\n",
        "    for _ in range(config.n_layer):\n",
        "        # Layer normalization parameters\n",
        "        ln_params = 2 * config.n_embd * 2  # Two layer norms per block, assuming both scale and bias\n",
        "        \n",
        "        # Self-attention parameters\n",
        "        attn_params = (config.n_embd * 3 * config.n_embd) + (config.n_embd * config.n_embd)  # QKV projections and output projection\n",
        "        if config.bias:\n",
        "            attn_params += 4 * config.n_embd  # Adding bias terms for QKV and output projection\n",
        "        \n",
        "        # MLP parameters\n",
        "        mlp_params = (config.n_embd * 4 * config.n_embd) + (4 * config.n_embd * config.n_embd)  # FC and proj layers\n",
        "        if config.bias:\n",
        "            mlp_params += 2 * 4 * config.n_embd  # Adding bias terms for FC and proj layers\n",
        "        \n",
        "        transformer_params += ln_params + attn_params + mlp_params\n",
        "    \n",
        "    # Total parameters (summing up token embedding, positional embedding, and transformer parameters)\n",
        "    total_params = token_embedding_params + position_embedding_params + transformer_params\n",
        "    return total_params\n",
        "\n",
        "# Example usage\n",
        "config = GPTConfig(n_layer=12, n_embd=768, vocab_size=)  # Example configuration with specified vocabulary size\n",
        "total_params = calculate_parameters(config)\n",
        "print(f\"Total Parameters: {total_params}\")\n"
      ],
      "metadata": {
        "id": "C0qejD7C6dXz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "nieCXObW8Xdn",
        "outputId": "852a3775-2f44-421c-b11c-3d4815cfa4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'n_layer_values' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a8a9aa50d5a3>\u001b[0m in \u001b[0;36m<cell line: 72>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0mn_embd_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mparams_n_layer_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_layer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_embd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_embd_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfixed_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0mparams_n_embd_expanded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_data_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n_embd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_embs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n_layer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layer_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfixed_vocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;31m# Calculate the maximum value among all plots for consistent y-axis limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Calculating the maximum parameter value across all configurations for consistent y-axis limits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'n_layer_values' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "srpOSRBV9R77"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}