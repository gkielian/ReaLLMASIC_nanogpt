{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gkielian/ReaLLMASIC_nanogpt/blob/master/NanoGPT_Quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<div class=\"mrkdown-google-sans\">\n",
        "\n",
        "# **GPU Quickstart**\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "swIfEfkhpCSZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Qfh9x6myI9g"
      },
      "source": [
        "<div class=\"mrkdown-google-sans\">\n",
        "\n",
        "### **Install NanoGPT GPU Dependencies**\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "!rm -rf nanoGPT_gpu\n",
        "!git clone https://github.com/ReaLLMASIC/nanoGPT.git nanoGPT_gpu\n",
        "%cd nanoGPT_gpu\n",
        "\n",
        "# check branch info\n",
        "!echo \"Cloned repository\"\n",
        "!git branch\n",
        "\n",
        "!ls\n",
        "\n",
        "!pip install --upgrade pip\n",
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install numpy transformers datasets tiktoken wandb tqdm tensorboard torchinfo\n"
      ],
      "metadata": {
        "id": "z9TWJn8PpJLg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc56c2c-ff97-4507-ff67-6afbcdacf3a1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "Cloning into 'nanoGPT_gpu'...\n",
            "remote: Enumerating objects: 4568, done.\u001b[K\n",
            "remote: Total 4568 (delta 0), reused 0 (delta 0), pack-reused 4568 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4568/4568), 38.85 MiB | 28.83 MiB/s, done.\n",
            "Resolving deltas: 100% (2746/2746), done.\n",
            "/root/nanoGPT_gpu\n",
            "Cloned repository\n",
            "* \u001b[32mmaster\u001b[m\n",
            "bench.py\t\t  gpt_conf.py\t     publications\t\t statistics_util\n",
            "colabs\t\t\t  huggingface_model  quantization\t\t steering_vector_util\n",
            "config\t\t\t  HW\t\t     README.md\t\t\t tests\n",
            "Contributing_Features.md  images\t     requirements_cpu.txt\t tf_np_golden_gen\n",
            "curriculum\t\t  inspect_ckpts.py   run_curriculum_learning.py  train.py\n",
            "data\t\t\t  LICENSE\t     run_experiments.py\t\t variations\n",
            "demos\t\t\t  model_info_util    run_vizier.py\t\t visualization_util\n",
            "documentation\t\t  model.py\t     sample.py\t\t\t visualize.py\n",
            "explorations\t\t  modules\t     softmax_sweep.py\n",
            "factorization_util\t  monitoring_util    start_tensorboard.sh\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.3.1)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.18.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.67.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za1OziLayNmh"
      },
      "source": [
        "<div class=\"mrkdown-google-sans\">\n",
        "\n",
        "### **Run GPU Training**\n",
        "\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-c73d8WyfIZ",
        "outputId": "6faffa34-5e92-419b-aeb6-1a61824c7d35"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --max_iters=2000 --out_dir=\"out-shakespeare\" --max_sample_tokens 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fbe6b5a-4d39-46d7-bc4d-7a3d6468da10",
        "id": "GvvoJVMXQ6x5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-11-19 00:59:16.550499: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-19 00:59:16.569455: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-19 00:59:16.575230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-19 00:59:16.589169: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-19 00:59:17.720191: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "seed:  1337\n",
            "seed offset:  0\n",
            "File data/shakespeare_char/meta.pkl copied to out-shakespeare\n",
            "resetting best val loss file\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "number of parameters: \u001b[1;36m10.\u001b[0m65M\n",
            "\u001b[1;4;35mHigh Level Parameters:\u001b[0m\n",
            "===========================================================================\n",
            "Layer (type:depth-idx)                             Param #\n",
            "===========================================================================\n",
            "GPT                                                --\n",
            "├─ModuleDict: 1-1                                  --\n",
            "│    └─Embedding: 2-1                              24,960\n",
            "│    └─Dropout: 2-2                                --\n",
            "│    └─ModuleList: 2-3                             --\n",
            "│    │    └─Block: 3-1                             1,770,240\n",
            "│    │    └─Block: 3-2                             1,770,240\n",
            "│    │    └─Block: 3-3                             1,770,240\n",
            "│    │    └─Block: 3-4                             1,770,240\n",
            "│    │    └─Block: 3-5                             1,770,240\n",
            "│    │    └─Block: 3-6                             1,770,240\n",
            "│    └─RMSNorm: 2-4                                384\n",
            "│    └─Embedding: 2-5                              98,304\n",
            "├─Linear: 1-2                                      24,960\n",
            "===========================================================================\n",
            "Total params: 10,770,048\n",
            "Trainable params: 10,770,048\n",
            "Non-trainable params: 0\n",
            "===========================================================================\n",
            "\u001b[1;4;32mSummary for Block 1:\u001b[0m\n",
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "Block                                    --\n",
            "├─RMSNorm: 1-1                           384\n",
            "├─RMSNorm: 1-2                           384\n",
            "├─CausalSelfAttention: 1-3               --\n",
            "│    └─WrappedLinear: 2-1                147,456\n",
            "│    └─WrappedLinear: 2-2                147,456\n",
            "│    └─WrappedLinear: 2-3                147,456\n",
            "│    └─WrappedLinear: 2-4                147,456\n",
            "│    └─Dropout: 2-5                      --\n",
            "│    └─Dropout: 2-6                      --\n",
            "├─MLP: 1-4                               --\n",
            "│    └─GELU_Config: 2-7                  --\n",
            "│    │    └─GELU: 3-1                    --\n",
            "│    └─WrappedLinear: 2-8                589,824\n",
            "│    └─WrappedLinear: 2-9                589,824\n",
            "│    └─Dropout: 2-10                     --\n",
            "=================================================================\n",
            "Total params: 1,770,240\n",
            "Trainable params: 1,770,240\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "\u001b[2m--------------------------------------------------\u001b[0m\n",
            "\u001b[33mtransformer: \u001b[0m\u001b[1;35mModuleDict\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mwte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m65\u001b[0m\u001b[33m, \u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mModuleList\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m    \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;36m6\u001b[0m\u001b[33m x \u001b[0m\u001b[1;35mBlock\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mattn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mCausalSelfAttention\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_proj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mattn_dropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mresid_dropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mmlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mMLP\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mactivation_variant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mGELU_Config\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m          \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mGELU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mapproximate\u001b[0m\u001b[33m=\u001b[0m\u001b[32m'none'\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_fc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m1536\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_proj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m1536\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m    \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mwpe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[33m, \u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[1;33m)\u001b[0m\n",
            "\u001b[33mlm_head: \u001b[0m\u001b[1;35mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m65\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[2m--------------------------------------------------\u001b[0m\n",
            "transformer: ModuleDict\n",
            "  wte.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m0.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m0.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mmlp.c_proj.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m1.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m1.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mmlp.c_proj.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m2.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m2.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mmlp.c_proj.weight\n",
            "  h.\u001b[1;36m3.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m3.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m3.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m3.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m3.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m3.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m3.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m3.\u001b[0mmlp.c_proj.weight\n",
            "  h.\u001b[1;36m4.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m4.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m4.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m4.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m4.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m4.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m4.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m4.\u001b[0mmlp.c_proj.weight\n",
            "  h.\u001b[1;36m5.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m5.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m5.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m5.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m5.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m5.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m5.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m5.\u001b[0mmlp.c_proj.weight\n",
            "  ln_f.gain\n",
            "  wpe.weight\n",
            "lm_head: Linear\n",
            "  weight\n",
            "num decayed parameter tensors: \u001b[1;36m38\u001b[0m, with \u001b[1;36m10\u001b[0m,\u001b[1;36m740\u001b[0m,\u001b[1;36m096\u001b[0m parameters\n",
            "num non-decayed parameter tensors: \u001b[1;36m13\u001b[0m, with \u001b[1;36m4\u001b[0m,\u001b[1;36m992\u001b[0m parameters\n",
            "using fused AdamW: \u001b[3;92mTrue\u001b[0m\n",
            "\u001b[2Kstep 0: train loss 4.3727, val loss 4.3620\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            ",MKHHP'SneJIfANAIqWw&d?E;aaenPt$EjzykbJ;n\n",
            "WuDsRzDxkYGk&hqA$U$$EFkgznKG,CEbBBVbNRXxv,dj.kmwJImmm\n",
            "NfOW\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 0: loss 4.3422, time 75387.15 ms, mfu -100.00%\n",
            "\u001b[2Kiter 10: loss 3.3287, time 565.38 ms, mfu 0.66%\n",
            "\u001b[2Kiter 20: loss 3.2345, time 566.41 ms, mfu 0.66%\n",
            "\u001b[2Kiter 30: loss 3.0173, time 569.88 ms, mfu 0.66%\n",
            "\u001b[2Kiter 40: loss 2.8938, time 572.10 ms, mfu 0.66%\n",
            "\u001b[2Kiter 50: loss 2.7892, time 574.16 ms, mfu 0.66%\n",
            "\u001b[2Kiter 60: loss 2.7108, time 583.09 ms, mfu 0.66%\n",
            "\u001b[2Kiter 70: loss 2.6630, time 585.92 ms, mfu 0.65%\n",
            "\u001b[2Kiter 80: loss 2.6252, time 586.91 ms, mfu 0.65%\n",
            "\u001b[2Kiter 90: loss 2.5714, time 580.16 ms, mfu 0.65%\n",
            "\u001b[2Kiter 100: loss 2.5634, time 573.88 ms, mfu 0.65%\n",
            "\u001b[2Kiter 110: loss 2.5536, time 576.43 ms, mfu 0.65%\n",
            "\u001b[2Kiter 120: loss 2.5296, time 574.24 ms, mfu 0.65%\n",
            "\u001b[2Kiter 130: loss 2.5135, time 571.23 ms, mfu 0.65%\n",
            "\u001b[2Kiter 140: loss 2.5051, time 570.58 ms, mfu 0.65%\n",
            "\u001b[2Kiter 150: loss 2.5169, time 575.08 ms, mfu 0.65%\n",
            "\u001b[2Kiter 160: loss 2.4982, time 571.94 ms, mfu 0.65%\n",
            "\u001b[2Kiter 170: loss 2.4771, time 572.53 ms, mfu 0.65%\n",
            "\u001b[2Kiter 180: loss 2.4708, time 575.20 ms, mfu 0.65%\n",
            "\u001b[2Kiter 190: loss 2.4596, time 571.11 ms, mfu 0.65%\n",
            "\u001b[2Kiter 200: loss 2.4501, time 572.18 ms, mfu 0.65%\n",
            "\u001b[2Kiter 210: loss 2.4220, time 575.12 ms, mfu 0.65%\n",
            "\u001b[2Kiter 220: loss 2.4568, time 572.42 ms, mfu 0.65%\n",
            "\u001b[2Kiter 230: loss 2.4392, time 577.40 ms, mfu 0.65%\n",
            "\u001b[2Kiter 240: loss 2.4473, time 575.69 ms, mfu 0.65%\n",
            "\u001b[2Kstep 250: train loss 2.4023, val loss 2.4265\n",
            "\u001b[2Ksaving checkpoint to out-shakespeare\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "Nasas! wus,a ago\n",
            "\n",
            "I hat mer cowhere?\n",
            "\n",
            "RAMABENCUK:\n",
            "Weay, soureato theshathafthardo is wincee wesh I b\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 250: loss 2.4230, time 78151.59 ms, mfu 0.59%\n",
            "\u001b[2Kiter 260: loss 2.4236, time 571.57 ms, mfu 0.59%\n",
            "\u001b[2Kiter 270: loss 2.4173, time 577.56 ms, mfu 0.60%\n",
            "\u001b[2Kiter 280: loss 2.4315, time 575.50 ms, mfu 0.60%\n",
            "\u001b[2Kiter 290: loss 2.3926, time 572.33 ms, mfu 0.61%\n",
            "\u001b[2Kiter 300: loss 2.4010, time 573.03 ms, mfu 0.61%\n",
            "\u001b[2Kiter 310: loss 2.3828, time 573.70 ms, mfu 0.62%\n",
            "\u001b[2Kiter 320: loss 2.3729, time 572.28 ms, mfu 0.62%\n",
            "\u001b[2Kiter 330: loss 2.3349, time 577.71 ms, mfu 0.62%\n",
            "\u001b[2Kiter 340: loss 2.3120, time 576.42 ms, mfu 0.62%\n",
            "\u001b[2Kiter 350: loss 2.3275, time 572.44 ms, mfu 0.63%\n",
            "\u001b[2Kiter 360: loss 2.2830, time 572.89 ms, mfu 0.63%\n",
            "\u001b[2Kiter 370: loss 2.2310, time 576.84 ms, mfu 0.63%\n",
            "\u001b[2Kiter 380: loss 2.1865, time 571.89 ms, mfu 0.63%\n",
            "\u001b[2Kiter 390: loss 2.1992, time 571.43 ms, mfu 0.63%\n",
            "\u001b[2Kiter 400: loss 2.1849, time 575.23 ms, mfu 0.64%\n",
            "\u001b[2Kiter 410: loss 2.1290, time 576.02 ms, mfu 0.64%\n",
            "\u001b[2Kiter 420: loss 2.1332, time 575.23 ms, mfu 0.64%\n",
            "\u001b[2Kiter 430: loss 2.1103, time 572.73 ms, mfu 0.64%\n",
            "\u001b[2Kiter 440: loss 2.0663, time 572.12 ms, mfu 0.64%\n",
            "\u001b[2Kiter 450: loss 2.0402, time 572.27 ms, mfu 0.64%\n",
            "\u001b[2Kiter 460: loss 1.9986, time 574.41 ms, mfu 0.64%\n",
            "\u001b[2Kiter 470: loss 2.0501, time 571.59 ms, mfu 0.64%\n",
            "\u001b[2Kiter 480: loss 1.9760, time 575.61 ms, mfu 0.64%\n",
            "\u001b[2Kiter 490: loss 1.9708, time 569.45 ms, mfu 0.64%\n",
            "\u001b[2Kstep 500: train loss 1.8538, val loss 1.9773\n",
            "\u001b[2Ksaving checkpoint to out-shakespeare\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "mos them, treast. Wil frour when m aus, myould theirgmine les,\n",
            "go dece specon maloveld ast to of as \n",
            "\u001b[2K\n",
            "\u001b[2Kiter 500: loss 1.9546, time 78402.78 ms, mfu 0.58%\n",
            "\u001b[2Kiter 510: loss 1.9511, time 571.97 ms, mfu 0.59%\n",
            "\u001b[2Kiter 520: loss 1.9235, time 577.07 ms, mfu 0.59%\n",
            "\u001b[2Kiter 530: loss 1.8986, time 577.24 ms, mfu 0.60%\n",
            "\u001b[2Kiter 540: loss 1.9263, time 578.64 ms, mfu 0.60%\n",
            "\u001b[2Kiter 550: loss 1.8606, time 576.42 ms, mfu 0.61%\n",
            "\u001b[2Kiter 560: loss 1.8656, time 570.79 ms, mfu 0.61%\n",
            "\u001b[2Kiter 570: loss 1.8468, time 576.21 ms, mfu 0.62%\n",
            "\u001b[2Kiter 580: loss 1.8253, time 574.90 ms, mfu 0.62%\n",
            "\u001b[2Kiter 590: loss 1.7763, time 574.58 ms, mfu 0.62%\n",
            "\u001b[2Kiter 600: loss 1.7918, time 570.59 ms, mfu 0.62%\n",
            "\u001b[2Kiter 610: loss 1.7995, time 571.48 ms, mfu 0.63%\n",
            "\u001b[2Kiter 620: loss 1.7819, time 571.98 ms, mfu 0.63%\n",
            "\u001b[2Kiter 630: loss 1.7577, time 571.69 ms, mfu 0.63%\n",
            "\u001b[2Kiter 640: loss 1.7083, time 571.14 ms, mfu 0.63%\n",
            "\u001b[2Kiter 650: loss 1.7298, time 574.53 ms, mfu 0.64%\n",
            "\u001b[2Kiter 660: loss 1.7278, time 574.05 ms, mfu 0.64%\n",
            "\u001b[2Kiter 670: loss 1.6712, time 575.39 ms, mfu 0.64%\n",
            "\u001b[2Kiter 680: loss 1.7191, time 575.65 ms, mfu 0.64%\n",
            "\u001b[2Kiter 690: loss 1.6735, time 575.47 ms, mfu 0.64%\n",
            "\u001b[2Kiter 700: loss 1.6898, time 575.93 ms, mfu 0.64%\n",
            "\u001b[2Kiter 710: loss 1.6509, time 574.03 ms, mfu 0.64%\n",
            "\u001b[2Kiter 720: loss 1.6272, time 578.30 ms, mfu 0.64%\n",
            "\u001b[2Kiter 730: loss 1.6135, time 579.04 ms, mfu 0.64%\n",
            "\u001b[2Kiter 740: loss 1.6074, time 577.02 ms, mfu 0.64%\n",
            "\u001b[2Kstep 750: train loss 1.5123, val loss 1.7099\n",
            "\u001b[2Ksaving checkpoint to out-shakespeare\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "\n",
            "SIORD Roncescy Muried, appey, yene\n",
            "For you ear be the enjusty that till.\n",
            "\n",
            "GLOUCET.\n",
            "\n",
            "KINGS:\n",
            "\n",
            "JETApon\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 750: loss 1.6006, time 78732.45 ms, mfu 0.58%\n",
            "\u001b[2Kiter 760: loss 1.6106, time 572.96 ms, mfu 0.59%\n",
            "\u001b[2Kiter 770: loss 1.5915, time 573.92 ms, mfu 0.59%\n",
            "\u001b[2Kiter 780: loss 1.5913, time 574.03 ms, mfu 0.60%\n",
            "\u001b[2Kiter 790: loss 1.5768, time 576.45 ms, mfu 0.60%\n",
            "\u001b[2Kiter 800: loss 1.5909, time 574.68 ms, mfu 0.61%\n",
            "\u001b[2Kiter 810: loss 1.5651, time 578.55 ms, mfu 0.61%\n",
            "\u001b[2Kiter 820: loss 1.5637, time 577.14 ms, mfu 0.61%\n",
            "\u001b[2Kiter 830: loss 1.5406, time 576.06 ms, mfu 0.62%\n",
            "\u001b[2Kiter 840: loss 1.5629, time 573.40 ms, mfu 0.62%\n",
            "\u001b[2Kiter 850: loss 1.5338, time 572.19 ms, mfu 0.62%\n",
            "\u001b[2Kiter 860: loss 1.5345, time 573.61 ms, mfu 0.63%\n",
            "\u001b[2Kiter 870: loss 1.5292, time 573.68 ms, mfu 0.63%\n",
            "\u001b[2Kiter 880: loss 1.5185, time 576.66 ms, mfu 0.63%\n",
            "\u001b[2Kiter 890: loss 1.5223, time 572.86 ms, mfu 0.63%\n",
            "\u001b[2Kiter 900: loss 1.5077, time 573.20 ms, mfu 0.63%\n",
            "\u001b[2Kiter 910: loss 1.4548, time 575.53 ms, mfu 0.64%\n",
            "\u001b[2Kiter 920: loss 1.4870, time 571.68 ms, mfu 0.64%\n",
            "\u001b[2Kiter 930: loss 1.4845, time 573.51 ms, mfu 0.64%\n",
            "\u001b[2Kiter 940: loss 1.4715, time 575.14 ms, mfu 0.64%\n",
            "\u001b[2Kiter 950: loss 1.4688, time 573.78 ms, mfu 0.64%\n",
            "\u001b[2Kiter 960: loss 1.4762, time 576.49 ms, mfu 0.64%\n",
            "\u001b[2Kiter 970: loss 1.4732, time 578.35 ms, mfu 0.64%\n",
            "\u001b[2Kiter 980: loss 1.4689, time 577.06 ms, mfu 0.64%\n",
            "\u001b[2Kiter 990: loss 1.4522, time 572.03 ms, mfu 0.64%\n",
            "\u001b[2Kstep 1000: train loss 1.3718, val loss 1.5840\n",
            "\u001b[2Ksaving checkpoint to out-shakespeare\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "Now? Ner go spiried, more anoral falow'd of,\n",
            "To coil layal of twas him! he manster no more;\n",
            "What I h\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 1000: loss 1.4430, time 78499.95 ms, mfu 0.58%\n",
            "\u001b[2Kiter 1010: loss 1.4514, time 576.97 ms, mfu 0.59%\n",
            "\u001b[2Kiter 1020: loss 1.4140, time 576.94 ms, mfu 0.59%\n",
            "\u001b[2Kiter 1030: loss 1.4437, time 577.91 ms, mfu 0.60%\n",
            "\u001b[2Kiter 1040: loss 1.4646, time 577.70 ms, mfu 0.60%\n",
            "\u001b[2Kiter 1050: loss 1.3977, time 572.94 ms, mfu 0.61%\n",
            "\u001b[2Kiter 1060: loss 1.4385, time 573.46 ms, mfu 0.61%\n",
            "\u001b[2Kiter 1070: loss 1.4340, time 571.37 ms, mfu 0.62%\n",
            "\u001b[2Kiter 1080: loss 1.4385, time 576.13 ms, mfu 0.62%\n",
            "\u001b[2Kiter 1090: loss 1.4475, time 574.33 ms, mfu 0.62%\n",
            "\u001b[2Kiter 1100: loss 1.4156, time 575.59 ms, mfu 0.62%\n",
            "\u001b[2Kiter 1110: loss 1.3952, time 575.74 ms, mfu 0.63%\n",
            "\u001b[2Kiter 1120: loss 1.4001, time 576.14 ms, mfu 0.63%\n",
            "\u001b[2Kiter 1130: loss 1.3877, time 574.23 ms, mfu 0.63%\n",
            "\u001b[2Kiter 1140: loss 1.3880, time 571.20 ms, mfu 0.63%\n",
            "\u001b[2Kiter 1150: loss 1.3981, time 575.78 ms, mfu 0.63%\n",
            "\u001b[2Kiter 1160: loss 1.4347, time 577.77 ms, mfu 0.64%\n",
            "\u001b[2Kiter 1170: loss 1.3967, time 576.16 ms, mfu 0.64%\n",
            "\u001b[2Kiter 1180: loss 1.4055, time 576.90 ms, mfu 0.64%\n",
            "\u001b[2Kiter 1190: loss 1.3700, time 574.67 ms, mfu 0.64%\n",
            "\u001b[2Kiter 1200: loss 1.3792, time 573.84 ms, mfu 0.64%\n",
            "\u001b[2Kiter 1210: loss 1.3581, time 572.43 ms, mfu 0.64%\n",
            "\u001b[2Kiter 1220: loss 1.3998, time 575.81 ms, mfu 0.64%\n",
            "\u001b[2Kiter 1230: loss 1.3933, time 575.41 ms, mfu 0.64%\n",
            "\u001b[2Kiter 1240: loss 1.3932, time 573.29 ms, mfu 0.64%\n",
            "\u001b[2Kstep 1250: train loss 1.2891, val loss 1.5295\n",
            "\u001b[2Ksaving checkpoint to out-shakespeare\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "When was it were we all the season, and Elbow\n",
            "That courtors where I; off nur clorned naing wonds;\n",
            "Fi\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 1250: loss 1.3620, time 79063.37 ms, mfu 0.58%\n",
            "\u001b[2Kiter 1260: loss 1.3747, time 574.18 ms, mfu 0.59%\n",
            "\u001b[2Kiter 1270: loss 1.3535, time 571.24 ms, mfu 0.59%\n",
            "\u001b[2Kiter 1280: loss 1.3467, time 577.60 ms, mfu 0.60%\n",
            "\u001b[2Kiter 1290: loss 1.3636, time 572.61 ms, mfu 0.60%\n",
            "\u001b[2Kiter 1300: loss 1.3938, time 577.07 ms, mfu 0.61%\n",
            "\u001b[2K\u001b[32mTraining...\u001b[0m \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m 37%\u001b[0m \u001b[36m0:21:04\u001b[0m\n",
            "\u001b[?25hTraceback (most recent call last):\n",
            "  File \"/root/nanoGPT_gpu/train.py\", line 1320, in <module>\n",
            "    main()\n",
            "  File \"/root/nanoGPT_gpu/train.py\", line 1310, in main\n",
            "    trainer.train()\n",
            "  File \"/root/nanoGPT_gpu/train.py\", line 1243, in train\n",
            "    self.scaler.scale(loss).backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 581, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsR_dPeVyVJ8"
      },
      "source": [
        "<div class=\"mrkdown-google-sans\">\n",
        "\n",
        "### **Run GPU Inference**\n",
        "\n",
        "Once training is complete, you can run inference to generate tokens based on any set of input tokens as a starting point.\n",
        "\n",
        "Some parameters exist to prevent repeat loops, e.g. '**temperature**'.\n",
        "\n",
        "'**temperature**' involves a bit of randomness (can change random_seed for different results), with lower values being more deterministic and higher values being more random.\n",
        "\n",
        "Try out a few values from temperature from 0.2 to 2.0.\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 sample.py --out_dir=\"out-shakespeare\" --num_samples=1 --temperature 0.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkqxTfU_yUrK",
        "outputId": "69a41a63-4768-4c7e-ccbb-f725be04f344"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root/nanoGPT_gpu/sample.py:175: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=args.device)\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "number of parameters: \u001b[1;36m10.\u001b[0m65M\n",
            "Loading meta from out-shakespeare/meta.pkl\u001b[33m...\u001b[0m\n",
            "\u001b[1;4;35mHigh Level Parameters:\u001b[0m\n",
            "===========================================================================\n",
            "Layer (type:depth-idx)                             Param #\n",
            "===========================================================================\n",
            "GPT                                                --\n",
            "├─ModuleDict: 1-1                                  --\n",
            "│    └─Embedding: 2-1                              24,960\n",
            "│    └─Dropout: 2-2                                --\n",
            "│    └─ModuleList: 2-3                             --\n",
            "│    │    └─Block: 3-1                             1,770,240\n",
            "│    │    └─Block: 3-2                             1,770,240\n",
            "│    │    └─Block: 3-3                             1,770,240\n",
            "│    │    └─Block: 3-4                             1,770,240\n",
            "│    │    └─Block: 3-5                             1,770,240\n",
            "│    │    └─Block: 3-6                             1,770,240\n",
            "│    └─RMSNorm: 2-4                                384\n",
            "│    └─Embedding: 2-5                              98,304\n",
            "├─Linear: 1-2                                      24,960\n",
            "===========================================================================\n",
            "Total params: 10,770,048\n",
            "Trainable params: 10,770,048\n",
            "Non-trainable params: 0\n",
            "===========================================================================\n",
            "\u001b[1;4;32mSummary for Block 1:\u001b[0m\n",
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "Block                                    --\n",
            "├─RMSNorm: 1-1                           384\n",
            "├─RMSNorm: 1-2                           384\n",
            "├─CausalSelfAttention: 1-3               --\n",
            "│    └─WrappedLinear: 2-1                147,456\n",
            "│    └─WrappedLinear: 2-2                147,456\n",
            "│    └─WrappedLinear: 2-3                147,456\n",
            "│    └─WrappedLinear: 2-4                147,456\n",
            "│    └─Dropout: 2-5                      --\n",
            "│    └─Dropout: 2-6                      --\n",
            "├─MLP: 1-4                               --\n",
            "│    └─GELU_Config: 2-7                  --\n",
            "│    │    └─GELU: 3-1                    --\n",
            "│    └─WrappedLinear: 2-8                589,824\n",
            "│    └─WrappedLinear: 2-9                589,824\n",
            "│    └─Dropout: 2-10                     --\n",
            "=================================================================\n",
            "Total params: 1,770,240\n",
            "Trainable params: 1,770,240\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "\u001b[2m--------------------------------------------------\u001b[0m\n",
            "\u001b[33mtransformer: \u001b[0m\u001b[1;35mModuleDict\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mwte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m65\u001b[0m\u001b[33m, \u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mModuleList\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m    \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;36m6\u001b[0m\u001b[33m x \u001b[0m\u001b[1;35mBlock\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mattn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mCausalSelfAttention\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_proj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mattn_dropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mresid_dropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mmlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mMLP\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mactivation_variant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mGELU_Config\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m          \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mGELU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mapproximate\u001b[0m\u001b[33m=\u001b[0m\u001b[32m'none'\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_fc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m1536\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_proj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m1536\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.0\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m    \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mwpe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m256\u001b[0m\u001b[33m, \u001b[0m\u001b[1;36m384\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[1;33m)\u001b[0m\n",
            "\u001b[33mlm_head: \u001b[0m\u001b[1;35mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m384\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m65\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[2m--------------------------------------------------\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;32mKING RICHARD II:\u001b[0m\n",
            "\u001b[1;32mShall be be made the way of the grave?\u001b[0m\n",
            "\n",
            "\u001b[1;32mNORTHUMBERLAND:\u001b[0m\n",
            "\u001b[1;32mNeither, I beseech you are away, and than a will our\u001b[0m\n",
            "\u001b[1;32mYour proof is fair of the will,\u001b[0m\n",
            "\u001b[1;32mWhich ever I in late this overtain.\u001b[0m\n",
            "\n",
            "\u001b[1;32mWARWICK:\u001b[0m\n",
            "\u001b[1;32mAy, and well in the little thou with a speak.\u001b[0m\n",
            "\n",
            "\u001b[1;32mQUEEN ELIZABETH:\u001b[0m\n",
            "\u001b[1;32mBrother, and whom I should that more not what ever so,\u001b[0m\n",
            "\u001b[1;32mSome our fair eyes now poor of his sun\u001b[0m\n",
            "\u001b[1;32mThan none first of some the flower in Montague.\u001b[0m\n",
            "\n",
            "\u001b[1;32mPRINCE EDWARD:\u001b[0m\n",
            "\u001b[1;32mThou art thy discarr'd by his grace honours.\u001b[0m\n",
            "\n",
            "\u001b[1;32mDUKE OF YORK:\u001b[0m\n",
            "\u001b[1;32mWho not can y\u001b[0m\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"mrkdown-google-sans\">\n",
        "\n",
        "# **Modular Arithmetic Exploration**\n",
        "\n",
        "One of the biggest questions we have in Transformer research is how we can transfer knowledge between domains.\n",
        "\n",
        "One of our hypothesis is whether the Transformer architecture can transfer prior knowledge over isomorphic systems.\n",
        "\n",
        "If successful, this could suggest that LLMs collect the distinct types of groups in their journey of modeling natural language.\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "-qDKA5e678N8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div class=\"mrkdown-google-sans\">\n",
        "\n",
        "## Testing Transformer Knowledge Transfer\n",
        "\n",
        "A convenient way to test this is with smaller models on things like modular arithmetic, which are much faster to train than even shakespeare_char\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "Mr8AdCOL_SmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "%cd nanoGPT_gpu/data/modular_addition\n",
        "!bash create_examples.sh\n",
        "!head data/*"
      ],
      "metadata": {
        "id": "AQ2FV2pP_Urj",
        "outputId": "ea880742-89f1-456b-f8c9-50b4deafd759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/root/nanoGPT_gpu/data/modular_addition\n",
            "1\n",
            "2\n",
            "4\n",
            "8\n",
            "16\n",
            "==> data/base_16.txt <==\n",
            "dda\n",
            "ffe\n",
            "2f1\n",
            "437\n",
            "eca\n",
            "4ae\n",
            "4c0\n",
            "880\n",
            "8f7\n",
            "c1d\n",
            "\n",
            "==> data/base_1.txt <==\n",
            "111111111111100011111111111110001111111111000000\n",
            "111111111111111011111111111111101111111111111100\n",
            "110000000000000011111111111111101000000000000000\n",
            "111100000000000011100000000000001111111000000000\n",
            "111111111111110011111111111100001111111111000000\n",
            "111100000000000011111111110000001111111111111100\n",
            "111100000000000011111111111100000000000000000000\n",
            "111111110000000011111111000000000000000000000000\n",
            "111111110000000011111111111111101111111000000000\n",
            "111111111111000010000000000000001111111111111000\n",
            "\n",
            "==> data/base_2.txt <==\n",
            "101110110101\n",
            "111111110111\n",
            "010011111000\n",
            "001011001110\n",
            "011100110101\n",
            "001001010111\n",
            "001000110000\n",
            "000100010000\n",
            "000111111110\n",
            "001110001011\n",
            "\n",
            "==> data/base_4.txt <==\n",
            "131322\n",
            "333323\n",
            "203310\n",
            "013031\n",
            "230322\n",
            "012223\n",
            "010300\n",
            "020200\n",
            "023331\n",
            "031013\n",
            "\n",
            "==> data/base_8.txt <==\n",
            "515121\n",
            "717161\n",
            "207110\n",
            "403070\n",
            "614121\n",
            "402161\n",
            "404100\n",
            "010100\n",
            "017170\n",
            "411051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "%cd nanoGPT_gpu\n",
        "%cd data/modular_addition\n",
        "!python3 prepare.py -i data/base_1.txt"
      ],
      "metadata": {
        "id": "ELfKccouA5L9",
        "outputId": "2422afd7-b9bb-4b4a-9ab6-b878ded6c709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/root/nanoGPT_gpu\n",
            "/root/nanoGPT_gpu/data/modular_addition\n",
            "Length of dataset: 12,544\n",
            "Unique chars: \n",
            "01\n",
            "Vocab size: 3\n",
            "Train tokens: 11,289\n",
            "Val tokens: 1,255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd\n",
        "%cd nanoGPT_gpu\n",
        "!python3 train.py --dataset modular_addition --n_layer 3 --n_head 3 --n_embd 192 --block_size 147 --max_sample_tokens 196"
      ],
      "metadata": {
        "id": "_VhiMZfvAsww",
        "outputId": "59edace1-d5f2-49bf-9db9-400ddc0b67ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/root\n",
            "/root/nanoGPT_gpu\n",
            "2024-11-19 01:32:34.713665: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-11-19 01:32:34.747481: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-11-19 01:32:34.757204: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-11-19 01:32:34.783651: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-11-19 01:32:36.130893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "seed:  1337\n",
            "seed offset:  0\n",
            "File data/modular_addition/meta.pkl copied to out\n",
            "resetting best val loss file\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "sliding window size: \u001b[3;35mNone\u001b[0m\n",
            "setting flash attn\n",
            "number of parameters: \u001b[1;36m1.\u001b[0m33M\n",
            "\u001b[1;4;35mHigh Level Parameters:\u001b[0m\n",
            "===========================================================================\n",
            "Layer (type:depth-idx)                             Param #\n",
            "===========================================================================\n",
            "GPT                                                --\n",
            "├─ModuleDict: 1-1                                  --\n",
            "│    └─Embedding: 2-1                              576\n",
            "│    └─Dropout: 2-2                                --\n",
            "│    └─ModuleList: 2-3                             --\n",
            "│    │    └─Block: 3-1                             442,752\n",
            "│    │    └─Block: 3-2                             442,752\n",
            "│    │    └─Block: 3-3                             442,752\n",
            "│    └─RMSNorm: 2-4                                192\n",
            "│    └─Embedding: 2-5                              28,224\n",
            "├─Linear: 1-2                                      576\n",
            "===========================================================================\n",
            "Total params: 1,357,824\n",
            "Trainable params: 1,357,824\n",
            "Non-trainable params: 0\n",
            "===========================================================================\n",
            "\u001b[1;4;32mSummary for Block 1:\u001b[0m\n",
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "Block                                    --\n",
            "├─RMSNorm: 1-1                           192\n",
            "├─RMSNorm: 1-2                           192\n",
            "├─CausalSelfAttention: 1-3               --\n",
            "│    └─WrappedLinear: 2-1                36,864\n",
            "│    └─WrappedLinear: 2-2                36,864\n",
            "│    └─WrappedLinear: 2-3                36,864\n",
            "│    └─WrappedLinear: 2-4                36,864\n",
            "│    └─Dropout: 2-5                      --\n",
            "│    └─Dropout: 2-6                      --\n",
            "├─MLP: 1-4                               --\n",
            "│    └─GELU_Config: 2-7                  --\n",
            "│    │    └─GELU: 3-1                    --\n",
            "│    └─WrappedLinear: 2-8                147,456\n",
            "│    └─WrappedLinear: 2-9                147,456\n",
            "│    └─Dropout: 2-10                     --\n",
            "=================================================================\n",
            "Total params: 442,752\n",
            "Trainable params: 442,752\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "\u001b[2m--------------------------------------------------\u001b[0m\n",
            "\u001b[33mtransformer: \u001b[0m\u001b[1;35mModuleDict\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mwte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[33m, \u001b[0m\u001b[1;36m192\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mdrop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mModuleList\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m    \u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;36m3\u001b[0m\u001b[33m x \u001b[0m\u001b[1;35mBlock\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mattn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mCausalSelfAttention\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_q\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_k\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_attn_v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_proj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mattn_dropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mresid_dropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mmlp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mMLP\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mactivation_variant\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mGELU_Config\u001b[0m\u001b[1;33m(\u001b[0m\n",
            "\u001b[33m          \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mactivation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mGELU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mapproximate\u001b[0m\u001b[33m=\u001b[0m\u001b[32m'none'\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_fc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m768\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mc_proj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mWrappedLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m768\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m        \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mdropout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mp\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;36m.2\u001b[0m\u001b[33m, \u001b[0m\u001b[33minplace\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m      \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m    \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mln_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mRMSNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[33m  \u001b[0m\u001b[1;33m(\u001b[0m\u001b[33mwpe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[33m: \u001b[0m\u001b[1;35mEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m147\u001b[0m\u001b[33m, \u001b[0m\u001b[1;36m192\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[1;33m)\u001b[0m\n",
            "\u001b[33mlm_head: \u001b[0m\u001b[1;35mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[33min_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m192\u001b[0m\u001b[33m, \u001b[0m\u001b[33mout_features\u001b[0m\u001b[33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[33m, \u001b[0m\u001b[33mbias\u001b[0m\u001b[33m=\u001b[0m\u001b[3;91mFalse\u001b[0m\u001b[1;33m)\u001b[0m\n",
            "\u001b[2m--------------------------------------------------\u001b[0m\n",
            "transformer: ModuleDict\n",
            "  wte.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m0.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m0.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m0.\u001b[0mmlp.c_proj.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m1.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m1.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m1.\u001b[0mmlp.c_proj.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mln_1.gain\n",
            "  h.\u001b[1;36m2.\u001b[0mln_2.gain\n",
            "  h.\u001b[1;36m2.\u001b[0mattn.c_attn_q.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mattn.c_attn_k.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mattn.c_attn_v.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mattn.c_proj.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mmlp.c_fc.weight\n",
            "  h.\u001b[1;36m2.\u001b[0mmlp.c_proj.weight\n",
            "  ln_f.gain\n",
            "  wpe.weight\n",
            "lm_head: Linear\n",
            "  weight\n",
            "num decayed parameter tensors: \u001b[1;36m20\u001b[0m, with \u001b[1;36m1\u001b[0m,\u001b[1;36m355\u001b[0m,\u001b[1;36m904\u001b[0m parameters\n",
            "num non-decayed parameter tensors: \u001b[1;36m7\u001b[0m, with \u001b[1;36m1\u001b[0m,\u001b[1;36m344\u001b[0m parameters\n",
            "using fused AdamW: \u001b[3;92mTrue\u001b[0m\n",
            "\u001b[2Kstep 0: train loss 0.6591, val loss 0.6572\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1111000\n",
            "\n",
            "\n",
            "1011000\n",
            "011110000000\n",
            "\n",
            "1100\n",
            "\n",
            "100011100001100111\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "\n",
            "000\n",
            "\n",
            "10\n",
            "\n",
            "\n",
            "0111011\n",
            "\n",
            "0\n",
            "\n",
            "\n",
            "1110000\n",
            "10000\n",
            "\n",
            "\n",
            "00\n",
            "\n",
            "\n",
            "0\n",
            "\n",
            "\n",
            "1000\n",
            "0\n",
            "11\n",
            "\n",
            "11\n",
            "\n",
            "1\n",
            "001\n",
            "\n",
            "000\n",
            "1\n",
            "10\n",
            "11001\n",
            "1\n",
            "100000111000000011000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "1110000000\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 0: loss 0.6829, time 4428.23 ms, mfu -100.00%\n",
            "\u001b[2Kiter 10: loss 0.5266, time 17.60 ms, mfu 1.54%\n",
            "\u001b[2Kiter 20: loss 0.3990, time 20.10 ms, mfu 1.52%\n",
            "\u001b[2Kiter 30: loss 0.3969, time 20.78 ms, mfu 1.50%\n",
            "\u001b[2Kiter 40: loss 0.3969, time 20.16 ms, mfu 1.48%\n",
            "\u001b[2Kiter 50: loss 0.3940, time 20.39 ms, mfu 1.47%\n",
            "\u001b[2Kiter 60: loss 0.4001, time 20.37 ms, mfu 1.45%\n",
            "\u001b[2Kiter 70: loss 0.4056, time 19.67 ms, mfu 1.45%\n",
            "\u001b[2Kiter 80: loss 0.4039, time 20.47 ms, mfu 1.43%\n",
            "\u001b[2Kiter 90: loss 0.3980, time 20.10 ms, mfu 1.43%\n",
            "\u001b[2Kiter 100: loss 0.3937, time 20.31 ms, mfu 1.42%\n",
            "\u001b[2Kiter 110: loss 0.3980, time 20.64 ms, mfu 1.41%\n",
            "\u001b[2Kiter 120: loss 0.3979, time 14.63 ms, mfu 1.45%\n",
            "\u001b[2Kiter 130: loss 0.4024, time 15.41 ms, mfu 1.48%\n",
            "\u001b[2Kiter 140: loss 0.3974, time 15.74 ms, mfu 1.51%\n",
            "\u001b[2Kiter 150: loss 0.3940, time 14.97 ms, mfu 1.54%\n",
            "\u001b[2Kiter 160: loss 0.3963, time 18.07 ms, mfu 1.53%\n",
            "\u001b[2Kiter 170: loss 0.3983, time 15.41 ms, mfu 1.56%\n",
            "\u001b[2Kiter 180: loss 0.3978, time 20.59 ms, mfu 1.53%\n",
            "\u001b[2Kiter 190: loss 0.3971, time 17.03 ms, mfu 1.54%\n",
            "\u001b[2Kiter 200: loss 0.3913, time 16.86 ms, mfu 1.54%\n",
            "\u001b[2Kiter 210: loss 0.3962, time 18.05 ms, mfu 1.54%\n",
            "\u001b[2Kiter 220: loss 0.3936, time 20.62 ms, mfu 1.52%\n",
            "\u001b[2Kiter 230: loss 0.3938, time 21.26 ms, mfu 1.49%\n",
            "\u001b[2Kiter 240: loss 0.3892, time 20.43 ms, mfu 1.48%\n",
            "\u001b[2Kstep 250: train loss 0.3880, val loss 0.3909\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "100000000000011100000\n",
            "11111111100\n",
            "11110000000\n",
            "11111111111111100001111111111100000001100001111110000000000000000000\n",
            "11111110000000100000000000011111111110000000000000\n",
            "111000000000000000000000000000\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 250: loss 0.3901, time 3537.40 ms, mfu 1.33%\n",
            "\u001b[2Kiter 260: loss 0.3860, time 20.44 ms, mfu 1.33%\n",
            "\u001b[2Kiter 270: loss 0.3802, time 20.09 ms, mfu 1.33%\n",
            "\u001b[2Kiter 280: loss 0.3685, time 20.59 ms, mfu 1.33%\n",
            "\u001b[2Kiter 290: loss 0.3574, time 20.64 ms, mfu 1.33%\n",
            "\u001b[2Kiter 300: loss 0.3590, time 20.68 ms, mfu 1.33%\n",
            "\u001b[2Kiter 310: loss 0.3452, time 20.58 ms, mfu 1.33%\n",
            "\u001b[2Kiter 320: loss 0.3500, time 21.95 ms, mfu 1.32%\n",
            "\u001b[2Kiter 330: loss 0.3361, time 20.52 ms, mfu 1.32%\n",
            "\u001b[2Kiter 340: loss 0.3310, time 20.38 ms, mfu 1.32%\n",
            "\u001b[2Kiter 350: loss 0.3307, time 20.49 ms, mfu 1.32%\n",
            "\u001b[2Kiter 360: loss 0.3267, time 20.04 ms, mfu 1.32%\n",
            "\u001b[2Kiter 370: loss 0.3221, time 23.17 ms, mfu 1.31%\n",
            "\u001b[2Kiter 380: loss 0.3132, time 20.47 ms, mfu 1.31%\n",
            "\u001b[2Kiter 390: loss 0.3034, time 20.32 ms, mfu 1.31%\n",
            "\u001b[2Kiter 400: loss 0.2954, time 20.47 ms, mfu 1.31%\n",
            "\u001b[2Kiter 410: loss 0.2960, time 20.59 ms, mfu 1.31%\n",
            "\u001b[2Kiter 420: loss 0.2875, time 20.68 ms, mfu 1.31%\n",
            "\u001b[2Kiter 430: loss 0.2866, time 20.15 ms, mfu 1.32%\n",
            "\u001b[2Kiter 440: loss 0.2796, time 20.40 ms, mfu 1.32%\n",
            "\u001b[2Kiter 450: loss 0.2808, time 21.44 ms, mfu 1.31%\n",
            "\u001b[2Kiter 460: loss 0.2704, time 20.22 ms, mfu 1.31%\n",
            "\u001b[2Kiter 470: loss 0.2659, time 20.69 ms, mfu 1.31%\n",
            "\u001b[2Kiter 480: loss 0.2701, time 20.71 ms, mfu 1.31%\n",
            "\u001b[2Kiter 490: loss 0.2651, time 20.51 ms, mfu 1.31%\n",
            "\u001b[2Kstep 500: train loss 0.2378, val loss 0.2384\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "1111000000000001111111100000011111111000000001101111111111000\n",
            "1110111111111111101111111000000011100000000000000\n",
            "110000000000000111111111111000011111111111111000\n",
            "1111111111000001111111110000000\n",
            "011\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 500: loss 0.2662, time 4028.24 ms, mfu 1.18%\n",
            "\u001b[2Kiter 510: loss 0.2608, time 19.05 ms, mfu 1.21%\n",
            "\u001b[2Kiter 520: loss 0.2576, time 20.60 ms, mfu 1.22%\n",
            "\u001b[2Kiter 530: loss 0.2606, time 20.41 ms, mfu 1.23%\n",
            "\u001b[2Kiter 540: loss 0.2540, time 20.67 ms, mfu 1.24%\n",
            "\u001b[2Kiter 550: loss 0.2470, time 21.34 ms, mfu 1.24%\n",
            "\u001b[2Kiter 560: loss 0.2468, time 20.68 ms, mfu 1.25%\n",
            "\u001b[2Kiter 570: loss 0.2455, time 20.74 ms, mfu 1.25%\n",
            "\u001b[2Kiter 580: loss 0.2404, time 20.64 ms, mfu 1.26%\n",
            "\u001b[2Kiter 590: loss 0.2401, time 20.50 ms, mfu 1.27%\n",
            "\u001b[2Kiter 600: loss 0.2413, time 20.62 ms, mfu 1.27%\n",
            "\u001b[2Kiter 610: loss 0.2447, time 20.87 ms, mfu 1.27%\n",
            "\u001b[2Kiter 620: loss 0.2340, time 20.53 ms, mfu 1.28%\n",
            "\u001b[2Kiter 630: loss 0.2355, time 20.53 ms, mfu 1.28%\n",
            "\u001b[2Kiter 640: loss 0.2338, time 20.77 ms, mfu 1.28%\n",
            "\u001b[2Kiter 650: loss 0.2292, time 20.18 ms, mfu 1.29%\n",
            "\u001b[2Kiter 660: loss 0.2342, time 19.82 ms, mfu 1.30%\n",
            "\u001b[2Kiter 670: loss 0.2368, time 20.10 ms, mfu 1.30%\n",
            "\u001b[2Kiter 680: loss 0.2338, time 20.55 ms, mfu 1.31%\n",
            "\u001b[2Kiter 690: loss 0.2303, time 20.37 ms, mfu 1.31%\n",
            "\u001b[2Kiter 700: loss 0.2260, time 20.75 ms, mfu 1.31%\n",
            "\u001b[2Kiter 710: loss 0.2252, time 20.66 ms, mfu 1.31%\n",
            "\u001b[2Kiter 720: loss 0.2267, time 20.35 ms, mfu 1.31%\n",
            "\u001b[2Kiter 730: loss 0.2280, time 19.87 ms, mfu 1.32%\n",
            "\u001b[2Kiter 740: loss 0.2252, time 19.47 ms, mfu 1.32%\n",
            "\u001b[2Kstep 750: train loss 0.2038, val loss 0.2032\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "11110000000000001100000000000000\n",
            "111111100000000011100000000000001111100000000000\n",
            "111111111000000011111111111111001111111111100000\n",
            "100000000000000011111111110000001100000000000000\n",
            "0000000000000000\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 750: loss 0.2237, time 3597.39 ms, mfu 1.19%\n",
            "\u001b[2Kiter 760: loss 0.2203, time 17.94 ms, mfu 1.22%\n",
            "\u001b[2Kiter 770: loss 0.2224, time 18.42 ms, mfu 1.25%\n",
            "\u001b[2Kiter 780: loss 0.2161, time 16.22 ms, mfu 1.29%\n",
            "\u001b[2Kiter 790: loss 0.2225, time 14.80 ms, mfu 1.34%\n",
            "\u001b[2Kiter 800: loss 0.2221, time 15.27 ms, mfu 1.39%\n",
            "\u001b[2Kiter 810: loss 0.2218, time 15.71 ms, mfu 1.42%\n",
            "\u001b[2Kiter 820: loss 0.2132, time 18.48 ms, mfu 1.43%\n",
            "\u001b[2Kiter 830: loss 0.2229, time 18.12 ms, mfu 1.43%\n",
            "\u001b[2Kiter 840: loss 0.2268, time 17.47 ms, mfu 1.45%\n",
            "\u001b[2Kiter 850: loss 0.2193, time 22.24 ms, mfu 1.42%\n",
            "\u001b[2Kiter 860: loss 0.2166, time 19.38 ms, mfu 1.42%\n",
            "\u001b[2Kiter 870: loss 0.2158, time 20.58 ms, mfu 1.41%\n",
            "\u001b[2Kiter 880: loss 0.2187, time 20.27 ms, mfu 1.40%\n",
            "\u001b[2Kiter 890: loss 0.2155, time 20.35 ms, mfu 1.40%\n",
            "\u001b[2Kiter 900: loss 0.2175, time 20.42 ms, mfu 1.39%\n",
            "\u001b[2Kiter 910: loss 0.2156, time 20.93 ms, mfu 1.38%\n",
            "\u001b[2Kiter 920: loss 0.2177, time 20.68 ms, mfu 1.37%\n",
            "\u001b[2Kiter 930: loss 0.2111, time 20.06 ms, mfu 1.37%\n",
            "\u001b[2Kiter 940: loss 0.2099, time 20.54 ms, mfu 1.37%\n",
            "\u001b[2Kiter 950: loss 0.2079, time 20.76 ms, mfu 1.36%\n",
            "\u001b[2Kiter 960: loss 0.2180, time 20.41 ms, mfu 1.36%\n",
            "\u001b[2Kiter 970: loss 0.2178, time 20.57 ms, mfu 1.35%\n",
            "\u001b[2Kiter 980: loss 0.2160, time 20.32 ms, mfu 1.35%\n",
            "\u001b[2Kiter 990: loss 0.2196, time 16.77 ms, mfu 1.38%\n",
            "\u001b[2Kstep 1000: train loss 0.2026, val loss 0.2025\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "1111111110000000111100000000000011110000000000001000000000000000\n",
            "11111100000000001111111111111110\n",
            "100000000000000011111111111000001111111111100000\n",
            "1111100000000000111111111111000011100000000000001\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 1000: loss 0.2160, time 3538.45 ms, mfu 1.24%\n",
            "\u001b[2Kiter 1010: loss 0.2128, time 20.37 ms, mfu 1.25%\n",
            "\u001b[2Kiter 1020: loss 0.2122, time 20.35 ms, mfu 1.26%\n",
            "\u001b[2Kiter 1030: loss 0.2125, time 20.49 ms, mfu 1.26%\n",
            "\u001b[2Kiter 1040: loss 0.2093, time 20.17 ms, mfu 1.27%\n",
            "\u001b[2Kiter 1050: loss 0.2088, time 20.73 ms, mfu 1.28%\n",
            "\u001b[2Kiter 1060: loss 0.2136, time 20.19 ms, mfu 1.28%\n",
            "\u001b[2Kiter 1070: loss 0.2121, time 20.49 ms, mfu 1.29%\n",
            "\u001b[2Kiter 1080: loss 0.2108, time 20.15 ms, mfu 1.29%\n",
            "\u001b[2Kiter 1090: loss 0.2162, time 20.62 ms, mfu 1.29%\n",
            "\u001b[2Kiter 1100: loss 0.2115, time 18.31 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1110: loss 0.2102, time 19.90 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1120: loss 0.2104, time 20.91 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1130: loss 0.2145, time 20.38 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1140: loss 0.2087, time 20.75 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1150: loss 0.2130, time 18.56 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1160: loss 0.2149, time 18.98 ms, mfu 1.34%\n",
            "\u001b[2Kiter 1170: loss 0.2072, time 15.65 ms, mfu 1.38%\n",
            "\u001b[2Kiter 1180: loss 0.2126, time 15.39 ms, mfu 1.42%\n",
            "\u001b[2Kiter 1190: loss 0.2131, time 19.32 ms, mfu 1.42%\n",
            "\u001b[2Kiter 1200: loss 0.2120, time 16.07 ms, mfu 1.44%\n",
            "\u001b[2Kiter 1210: loss 0.2066, time 21.08 ms, mfu 1.43%\n",
            "\u001b[2Kiter 1220: loss 0.2073, time 18.32 ms, mfu 1.43%\n",
            "\u001b[2Kiter 1230: loss 0.2107, time 15.90 ms, mfu 1.46%\n",
            "\u001b[2Kiter 1240: loss 0.2071, time 18.42 ms, mfu 1.46%\n",
            "\u001b[2Kstep 1250: train loss 0.1983, val loss 0.2006\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "111111000000000011111111000000001111111000000000\n",
            "11110000000000001111111111110\n",
            "100111111111111000\n",
            "1111111100000000111111111111100011111111111111101111111111100000\n",
            "111111111110000000000000000000000\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 1250: loss 0.2071, time 3634.12 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1260: loss 0.2046, time 20.32 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1270: loss 0.2086, time 18.79 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1280: loss 0.2142, time 20.41 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1290: loss 0.2065, time 20.62 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1300: loss 0.2059, time 20.33 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1310: loss 0.2036, time 20.27 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1320: loss 0.2112, time 20.30 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1330: loss 0.2085, time 20.38 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1340: loss 0.2059, time 20.58 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1350: loss 0.2060, time 19.99 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1360: loss 0.2107, time 20.19 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1370: loss 0.2107, time 20.24 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1380: loss 0.2069, time 20.21 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1390: loss 0.2135, time 20.55 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1400: loss 0.2055, time 20.25 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1410: loss 0.2037, time 20.37 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1420: loss 0.2066, time 20.37 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1430: loss 0.2028, time 19.86 ms, mfu 1.34%\n",
            "\u001b[2Kiter 1440: loss 0.2046, time 22.13 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1450: loss 0.2054, time 23.42 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1460: loss 0.2014, time 20.19 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1470: loss 0.2054, time 20.38 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1480: loss 0.2043, time 20.37 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1490: loss 0.2054, time 20.44 ms, mfu 1.32%\n",
            "\u001b[2Kstep 1500: train loss 0.1964, val loss 0.1993\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "111100000000000011111111111111101111111111110000\n",
            "111000000000000011000000000000001111100000000000\n",
            "111111111110000011111111111100001111000000000000\n",
            "1100000000000000111111100000000011111111111111110\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 1500: loss 0.2009, time 4100.46 ms, mfu 1.19%\n",
            "\u001b[2Kiter 1510: loss 0.2067, time 21.75 ms, mfu 1.19%\n",
            "\u001b[2Kiter 1520: loss 0.2104, time 20.37 ms, mfu 1.21%\n",
            "\u001b[2Kiter 1530: loss 0.2071, time 20.32 ms, mfu 1.22%\n",
            "\u001b[2Kiter 1540: loss 0.2067, time 20.36 ms, mfu 1.23%\n",
            "\u001b[2Kiter 1550: loss 0.2044, time 20.20 ms, mfu 1.24%\n",
            "\u001b[2Kiter 1560: loss 0.2041, time 20.38 ms, mfu 1.25%\n",
            "\u001b[2Kiter 1570: loss 0.2034, time 20.38 ms, mfu 1.26%\n",
            "\u001b[2Kiter 1580: loss 0.2054, time 20.20 ms, mfu 1.27%\n",
            "\u001b[2Kiter 1590: loss 0.2081, time 20.46 ms, mfu 1.27%\n",
            "\u001b[2Kiter 1600: loss 0.2026, time 19.30 ms, mfu 1.29%\n",
            "\u001b[2Kiter 1610: loss 0.2024, time 20.22 ms, mfu 1.29%\n",
            "\u001b[2Kiter 1620: loss 0.2063, time 20.77 ms, mfu 1.29%\n",
            "\u001b[2Kiter 1630: loss 0.2065, time 20.31 ms, mfu 1.30%\n",
            "\u001b[2Kiter 1640: loss 0.2045, time 20.12 ms, mfu 1.30%\n",
            "\u001b[2Kiter 1650: loss 0.2009, time 20.29 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1660: loss 0.2029, time 20.35 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1670: loss 0.2073, time 20.37 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1680: loss 0.2079, time 19.97 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1690: loss 0.2022, time 19.99 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1700: loss 0.2014, time 19.81 ms, mfu 1.32%\n",
            "\u001b[2Kiter 1710: loss 0.2064, time 20.12 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1720: loss 0.2000, time 20.38 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1730: loss 0.2034, time 20.23 ms, mfu 1.33%\n",
            "\u001b[2Kiter 1740: loss 0.2061, time 20.16 ms, mfu 1.33%\n",
            "\u001b[2Kstep 1750: train loss 0.1962, val loss 0.1999\n",
            "\u001b[2Kiter 1750: loss 0.2048, time 3001.56 ms, mfu 1.20%\n",
            "\u001b[2Kiter 1760: loss 0.2047, time 21.43 ms, mfu 1.20%\n",
            "\u001b[2Kiter 1770: loss 0.2014, time 20.35 ms, mfu 1.22%\n",
            "\u001b[2Kiter 1780: loss 0.1989, time 20.33 ms, mfu 1.23%\n",
            "\u001b[2Kiter 1790: loss 0.2036, time 22.29 ms, mfu 1.23%\n",
            "\u001b[2Kiter 1800: loss 0.2029, time 20.23 ms, mfu 1.24%\n",
            "\u001b[2Kiter 1810: loss 0.2053, time 20.40 ms, mfu 1.25%\n",
            "\u001b[2Kiter 1820: loss 0.2031, time 20.37 ms, mfu 1.26%\n",
            "\u001b[2Kiter 1830: loss 0.2034, time 19.49 ms, mfu 1.27%\n",
            "\u001b[2Kiter 1840: loss 0.2033, time 16.03 ms, mfu 1.31%\n",
            "\u001b[2Kiter 1850: loss 0.2008, time 16.36 ms, mfu 1.35%\n",
            "\u001b[2Kiter 1860: loss 0.2033, time 17.53 ms, mfu 1.37%\n",
            "\u001b[2Kiter 1870: loss 0.2032, time 14.85 ms, mfu 1.41%\n",
            "\u001b[2Kiter 1880: loss 0.2048, time 15.16 ms, mfu 1.45%\n",
            "\u001b[2Kiter 1890: loss 0.2054, time 19.75 ms, mfu 1.44%\n",
            "\u001b[2Kiter 1900: loss 0.2048, time 20.73 ms, mfu 1.43%\n",
            "\u001b[2Kiter 1910: loss 0.2014, time 18.42 ms, mfu 1.43%\n",
            "\u001b[2Kiter 1920: loss 0.2048, time 17.74 ms, mfu 1.44%\n",
            "\u001b[2Kiter 1930: loss 0.2010, time 20.47 ms, mfu 1.43%\n",
            "\u001b[2Kiter 1940: loss 0.2040, time 19.72 ms, mfu 1.43%\n",
            "\u001b[2Kiter 1950: loss 0.2027, time 18.90 ms, mfu 1.43%\n",
            "\u001b[2Kiter 1960: loss 0.2031, time 20.28 ms, mfu 1.42%\n",
            "\u001b[2Kiter 1970: loss 0.2039, time 20.21 ms, mfu 1.41%\n",
            "\u001b[2Kiter 1980: loss 0.2002, time 20.46 ms, mfu 1.40%\n",
            "\u001b[2Kiter 1990: loss 0.2015, time 20.32 ms, mfu 1.39%\n",
            "\u001b[2Kstep 2000: train loss 0.1941, val loss 0.1967\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "011100000000000011111111100000001111111111100000\n",
            "111111110000000011110000000000001111111111000000\n",
            "110000000000000011111111111111101111111111110000\n",
            "111111111111100011111000000000001000000000000000\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 2000: loss 0.1997, time 3582.12 ms, mfu 1.26%\n",
            "\u001b[2Kiter 2010: loss 0.2031, time 22.76 ms, mfu 1.25%\n",
            "\u001b[2Kiter 2020: loss 0.2029, time 19.87 ms, mfu 1.26%\n",
            "\u001b[2Kiter 2030: loss 0.2042, time 20.10 ms, mfu 1.27%\n",
            "\u001b[2Kiter 2040: loss 0.1983, time 20.25 ms, mfu 1.28%\n",
            "\u001b[2Kiter 2050: loss 0.2033, time 20.24 ms, mfu 1.28%\n",
            "\u001b[2Kiter 2060: loss 0.1999, time 20.10 ms, mfu 1.29%\n",
            "\u001b[2Kiter 2070: loss 0.2022, time 22.03 ms, mfu 1.28%\n",
            "\u001b[2Kiter 2080: loss 0.2007, time 18.98 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2090: loss 0.2002, time 20.24 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2100: loss 0.2017, time 21.04 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2110: loss 0.2008, time 20.32 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2120: loss 0.2004, time 20.41 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2130: loss 0.1997, time 20.69 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2140: loss 0.2001, time 20.16 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2150: loss 0.2022, time 20.32 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2160: loss 0.1982, time 20.39 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2170: loss 0.2022, time 20.37 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2180: loss 0.2025, time 20.26 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2190: loss 0.2029, time 20.99 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2200: loss 0.1970, time 20.36 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2210: loss 0.2002, time 20.32 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2220: loss 0.1992, time 20.25 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2230: loss 0.1970, time 20.04 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2240: loss 0.1980, time 16.07 ms, mfu 1.36%\n",
            "\u001b[2Kstep 2250: train loss 0.1892, val loss 0.1951\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "111111111000000011111110000000001111110000000000\n",
            "100000000000000011111111110000001111111111110000\n",
            "111111111000000011100000000000000000000000000000\n",
            "111100000000000011111000000000001000000000000000\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 2250: loss 0.1990, time 3953.11 ms, mfu 1.23%\n",
            "\u001b[2Kiter 2260: loss 0.1989, time 20.22 ms, mfu 1.24%\n",
            "\u001b[2Kiter 2270: loss 0.1991, time 20.31 ms, mfu 1.25%\n",
            "\u001b[2Kiter 2280: loss 0.1972, time 19.07 ms, mfu 1.26%\n",
            "\u001b[2Kiter 2290: loss 0.1975, time 20.35 ms, mfu 1.27%\n",
            "\u001b[2Kiter 2300: loss 0.1954, time 20.36 ms, mfu 1.28%\n",
            "\u001b[2Kiter 2310: loss 0.1968, time 20.12 ms, mfu 1.28%\n",
            "\u001b[2Kiter 2320: loss 0.2000, time 20.23 ms, mfu 1.29%\n",
            "\u001b[2Kiter 2330: loss 0.1944, time 20.34 ms, mfu 1.29%\n",
            "\u001b[2Kiter 2340: loss 0.1910, time 20.98 ms, mfu 1.29%\n",
            "\u001b[2Kiter 2350: loss 0.1962, time 20.32 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2360: loss 0.1995, time 20.29 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2370: loss 0.1938, time 20.33 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2380: loss 0.1938, time 20.42 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2390: loss 0.1951, time 20.17 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2400: loss 0.1926, time 20.36 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2410: loss 0.1904, time 20.33 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2420: loss 0.1922, time 20.40 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2430: loss 0.1919, time 19.96 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2440: loss 0.1930, time 20.49 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2450: loss 0.1904, time 20.26 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2460: loss 0.1938, time 20.35 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2470: loss 0.1907, time 20.53 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2480: loss 0.1921, time 19.33 ms, mfu 1.33%\n",
            "\u001b[2Kiter 2490: loss 0.1885, time 20.01 ms, mfu 1.33%\n",
            "\u001b[2Kstep 2500: train loss 0.1798, val loss 0.1859\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "111111110000000011111111111111101111100000000000\n",
            "100000000000000011111100000000001111111100000000\n",
            "100000000000000011100000000000001111100000000000\n",
            "111111100000000011111111100000001000000000000000\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 2500: loss 0.1908, time 3720.61 ms, mfu 1.20%\n",
            "\u001b[2Kiter 2510: loss 0.1918, time 14.95 ms, mfu 1.26%\n",
            "\u001b[2Kiter 2520: loss 0.1907, time 17.58 ms, mfu 1.29%\n",
            "\u001b[2Kiter 2530: loss 0.1884, time 20.33 ms, mfu 1.29%\n",
            "\u001b[2Kiter 2540: loss 0.1872, time 18.45 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2550: loss 0.1872, time 20.13 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2560: loss 0.1894, time 18.79 ms, mfu 1.33%\n",
            "\u001b[2Kiter 2570: loss 0.1895, time 17.61 ms, mfu 1.35%\n",
            "\u001b[2Kiter 2580: loss 0.1913, time 17.23 ms, mfu 1.37%\n",
            "\u001b[2Kiter 2590: loss 0.1888, time 21.91 ms, mfu 1.36%\n",
            "\u001b[2Kiter 2600: loss 0.1871, time 20.20 ms, mfu 1.36%\n",
            "\u001b[2Kiter 2610: loss 0.1900, time 22.07 ms, mfu 1.34%\n",
            "\u001b[2Kiter 2620: loss 0.1851, time 20.63 ms, mfu 1.34%\n",
            "\u001b[2Kiter 2630: loss 0.1899, time 19.81 ms, mfu 1.34%\n",
            "\u001b[2Kiter 2640: loss 0.1922, time 17.83 ms, mfu 1.36%\n",
            "\u001b[2Kiter 2650: loss 0.1874, time 20.31 ms, mfu 1.36%\n",
            "\u001b[2Kiter 2660: loss 0.1828, time 20.44 ms, mfu 1.36%\n",
            "\u001b[2Kiter 2670: loss 0.1868, time 20.77 ms, mfu 1.35%\n",
            "\u001b[2Kiter 2680: loss 0.1863, time 20.61 ms, mfu 1.35%\n",
            "\u001b[2Kiter 2690: loss 0.1888, time 20.35 ms, mfu 1.35%\n",
            "\u001b[2Kiter 2700: loss 0.1910, time 20.28 ms, mfu 1.34%\n",
            "\u001b[2Kiter 2710: loss 0.1849, time 20.40 ms, mfu 1.34%\n",
            "\u001b[2Kiter 2720: loss 0.1870, time 19.33 ms, mfu 1.35%\n",
            "\u001b[2Kiter 2730: loss 0.1870, time 19.03 ms, mfu 1.36%\n",
            "\u001b[2Kiter 2740: loss 0.1894, time 20.33 ms, mfu 1.35%\n",
            "\u001b[2Kstep 2750: train loss 0.1778, val loss 0.1851\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "110000000000000011111100000000001111110000000000\n",
            "100000000000000011000000000000001100000000000000\n",
            "111100000000000011111111111111001111111111111000\n",
            "110000000000000011111111111111001111111111110000\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 2750: loss 0.1863, time 3556.84 ms, mfu 1.22%\n",
            "\u001b[2Kiter 2760: loss 0.1839, time 21.61 ms, mfu 1.22%\n",
            "\u001b[2Kiter 2770: loss 0.1877, time 20.39 ms, mfu 1.23%\n",
            "\u001b[2Kiter 2780: loss 0.1890, time 20.59 ms, mfu 1.24%\n",
            "\u001b[2Kiter 2790: loss 0.1876, time 20.86 ms, mfu 1.25%\n",
            "\u001b[2Kiter 2800: loss 0.1854, time 20.50 ms, mfu 1.26%\n",
            "\u001b[2Kiter 2810: loss 0.1877, time 19.07 ms, mfu 1.27%\n",
            "\u001b[2Kiter 2820: loss 0.1847, time 18.82 ms, mfu 1.29%\n",
            "\u001b[2Kiter 2830: loss 0.1837, time 20.38 ms, mfu 1.29%\n",
            "\u001b[2Kiter 2840: loss 0.1831, time 20.55 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2850: loss 0.1829, time 19.80 ms, mfu 1.30%\n",
            "\u001b[2Kiter 2860: loss 0.1845, time 20.46 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2870: loss 0.1842, time 20.37 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2880: loss 0.1861, time 20.48 ms, mfu 1.31%\n",
            "\u001b[2Kiter 2890: loss 0.1843, time 19.03 ms, mfu 1.32%\n",
            "\u001b[2Kiter 2900: loss 0.1878, time 17.52 ms, mfu 1.34%\n",
            "\u001b[2Kiter 2910: loss 0.1839, time 15.49 ms, mfu 1.38%\n",
            "\u001b[2Kiter 2920: loss 0.1861, time 21.14 ms, mfu 1.37%\n",
            "\u001b[2Kiter 2930: loss 0.1832, time 17.81 ms, mfu 1.39%\n",
            "\u001b[2Kiter 2940: loss 0.1833, time 20.93 ms, mfu 1.38%\n",
            "\u001b[2Kiter 2950: loss 0.1806, time 24.44 ms, mfu 1.35%\n",
            "\u001b[2Kiter 2960: loss 0.1861, time 16.31 ms, mfu 1.38%\n",
            "\u001b[2Kiter 2970: loss 0.1809, time 17.11 ms, mfu 1.40%\n",
            "\u001b[2Kiter 2980: loss 0.1868, time 20.09 ms, mfu 1.40%\n",
            "\u001b[2Kiter 2990: loss 0.1831, time 18.55 ms, mfu 1.40%\n",
            "\u001b[2Kstep 3000: train loss 0.1729, val loss 0.1836\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "111111100000000011111111000000001000000000000000\n",
            "111110000000000011000000000000001111111100000000\n",
            "110000000000000011100000000000001111000000000000\n",
            "111111111111110010000000000000001111111111110000\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 3000: loss 0.1808, time 3518.73 ms, mfu 1.26%\n",
            "\u001b[2Kiter 3010: loss 0.1883, time 21.30 ms, mfu 1.27%\n",
            "\u001b[2Kiter 3020: loss 0.1865, time 20.40 ms, mfu 1.27%\n",
            "\u001b[2Kiter 3030: loss 0.1844, time 21.61 ms, mfu 1.27%\n",
            "\u001b[2Kiter 3040: loss 0.1824, time 20.91 ms, mfu 1.27%\n",
            "\u001b[2Kiter 3050: loss 0.1821, time 20.35 ms, mfu 1.28%\n",
            "\u001b[2Kiter 3060: loss 0.1801, time 20.51 ms, mfu 1.28%\n",
            "\u001b[2Kiter 3070: loss 0.1815, time 20.77 ms, mfu 1.29%\n",
            "\u001b[2Kiter 3080: loss 0.1812, time 19.59 ms, mfu 1.30%\n",
            "\u001b[2Kiter 3090: loss 0.1786, time 20.43 ms, mfu 1.30%\n",
            "\u001b[2Kiter 3100: loss 0.1823, time 20.42 ms, mfu 1.30%\n",
            "\u001b[2Kiter 3110: loss 0.1790, time 20.05 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3120: loss 0.1845, time 20.53 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3130: loss 0.1791, time 20.19 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3140: loss 0.1812, time 20.46 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3150: loss 0.1783, time 20.81 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3160: loss 0.1832, time 20.43 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3170: loss 0.1840, time 20.40 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3180: loss 0.1833, time 20.44 ms, mfu 1.32%\n",
            "\u001b[2Kiter 3190: loss 0.1778, time 20.52 ms, mfu 1.32%\n",
            "\u001b[2Kiter 3200: loss 0.1769, time 18.45 ms, mfu 1.33%\n",
            "\u001b[2Kiter 3210: loss 0.1818, time 20.46 ms, mfu 1.33%\n",
            "\u001b[2Kiter 3220: loss 0.1800, time 20.46 ms, mfu 1.33%\n",
            "\u001b[2Kiter 3230: loss 0.1792, time 20.52 ms, mfu 1.33%\n",
            "\u001b[2Kiter 3240: loss 0.1801, time 20.36 ms, mfu 1.33%\n",
            "\u001b[2Kstep 3250: train loss 0.1683, val loss 0.1810\n",
            "\u001b[2Ksaving checkpoint to out\n",
            "\u001b[2KStart tokens:\n",
            "\n",
            "\n",
            "\u001b[2K\n",
            "\u001b[2KSampled text:\n",
            "\n",
            "11100000000000001111100000000000111111000000000\n",
            "111111111110000010000000000000001111111111000000\n",
            "111111111100000011111100000000001100000000000000\n",
            "111111111111000011100000000000001110000000000000\n",
            "1\n",
            "\u001b[2K\n",
            "\u001b[2Kiter 3250: loss 0.1813, time 4015.76 ms, mfu 1.20%\n",
            "\u001b[2Kiter 3260: loss 0.1788, time 20.32 ms, mfu 1.21%\n",
            "\u001b[2Kiter 3270: loss 0.1775, time 20.06 ms, mfu 1.23%\n",
            "\u001b[2Kiter 3280: loss 0.1802, time 20.27 ms, mfu 1.24%\n",
            "\u001b[2Kiter 3290: loss 0.1806, time 20.46 ms, mfu 1.25%\n",
            "\u001b[2Kiter 3300: loss 0.1804, time 20.46 ms, mfu 1.25%\n",
            "\u001b[2Kiter 3310: loss 0.1796, time 20.11 ms, mfu 1.26%\n",
            "\u001b[2Kiter 3320: loss 0.1773, time 20.42 ms, mfu 1.27%\n",
            "\u001b[2Kiter 3330: loss 0.1802, time 20.98 ms, mfu 1.27%\n",
            "\u001b[2Kiter 3340: loss 0.1780, time 20.38 ms, mfu 1.28%\n",
            "\u001b[2Kiter 3350: loss 0.1816, time 23.87 ms, mfu 1.26%\n",
            "\u001b[2Kiter 3360: loss 0.1795, time 20.45 ms, mfu 1.27%\n",
            "\u001b[2Kiter 3370: loss 0.1774, time 20.44 ms, mfu 1.28%\n",
            "\u001b[2Kiter 3380: loss 0.1799, time 20.14 ms, mfu 1.28%\n",
            "\u001b[2Kiter 3390: loss 0.1811, time 20.53 ms, mfu 1.29%\n",
            "\u001b[2Kiter 3400: loss 0.1797, time 19.75 ms, mfu 1.29%\n",
            "\u001b[2Kiter 3410: loss 0.1772, time 20.43 ms, mfu 1.30%\n",
            "\u001b[2Kiter 3420: loss 0.1760, time 21.79 ms, mfu 1.29%\n",
            "\u001b[2Kiter 3430: loss 0.1768, time 20.47 ms, mfu 1.30%\n",
            "\u001b[2Kiter 3440: loss 0.1755, time 20.61 ms, mfu 1.30%\n",
            "\u001b[2Kiter 3450: loss 0.1777, time 20.61 ms, mfu 1.30%\n",
            "\u001b[2Kiter 3460: loss 0.1810, time 20.41 ms, mfu 1.30%\n",
            "\u001b[2Kiter 3470: loss 0.1843, time 20.16 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3480: loss 0.1805, time 20.39 ms, mfu 1.31%\n",
            "\u001b[2Kiter 3490: loss 0.1789, time 17.35 ms, mfu 1.33%\n",
            "\u001b[2Kstep 3500: train loss 0.1662, val loss 0.1821\n",
            "\u001b[2Kiter 3500: loss 0.1798, time 3006.90 ms, mfu 1.20%\n",
            "\u001b[2K\u001b[32mTraining...\u001b[0m \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WwdyRdiIDDfB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}